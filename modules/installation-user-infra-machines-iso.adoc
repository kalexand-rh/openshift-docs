have// Module included in the following assemblies:
//
// * installing/installing_bare_metal/installing-bare-metal.adoc
// * installing/installing_restricted_networks/installing-restricted-networks-bare-metal.adoc
// * installing/installing_ibm_z/installing-ibm-z.adoc

[id="installation-user-infra-machines-iso_{context}"]
= Creating {op-system-first} machines

Before you install a cluster on z/VM guest virtual machines,
you must install Red Hat Enterprise Linux CoreOS on those virtual machines.

.Prerequisites
ifeval::["{context}" == "installing-ibm-z"]
* An FTP server running on your provisioning machine that is accessible to the machines you create.
endif::[]

* An FTP server on your provisioning machine that
the machines that you create can access.

.Procedure
. Log in to Linux on your provisioning machine.
. Download the Red Hat Enterprise Linux CoreOS installation files from https://mirror.openshift.com/pub/openshift-v4/s390x/dependencies/rhcos/4.2/latest/
+
[IMPORTANT]
====
The {op-system} images might not change with every release of {product-title}.
You must download images with the highest version that is less than or equal
to the {product-title} version that you install. Use the image versions
that match your {product-title} version if they are available.
====
+
Download the following files:

* The initramfs: `rhcos-<version>-installer-initramfs.img`
* The kernel: `rhcos-<version>-installer-kernel`
* The operating system image for the disk on which you want to install RHCOS. This type can differ by virtual machine:
+
`rhcos-<version>-dasd.raw.gz` for DASD
+
`rhcos-<version>-metal-bios.raw.gz` for FCP

. Create parameter files. The following parameters are specific for a particular virtual machine:
** coreos.inst.install_dev=
specify `dasda` for a DASD installation, or `sda` for FCP. Note that FCP requires zfcp.allow_lun_scan=0. 
** rd.dasd=
specifies the DASD where RHCOS is to be installed.
** rd.zfcp=<adapter>,<wwpn>,<lun>
specifies the FCP disk where RHCOS is to be installed.   
** ip=
+
has 7 entries; the first is the IP address for the machine, the second can be empty, the third is the gateway, the fourth is the netmask, the fifth is in the form hostname.domainname; leave this blank to let RHCOS decide, the sixth is the network interface name; leave blank to let RHCOS decide, and the seventh can be left blank if no DHCP is used.
** coreos.inst.ignition_url=
+
gets the specific ignition file according to the machine role. The bootstrap machine ignition file is
called bootstrap-0, the master ignition files are numbered 0 through 2, the worker ignition files from 0
upwards. All other parameters can stay as they are.
+
Example parameter file, bootstrap-0.parm, for the bootstrap machine:
+
`rd.neednet=1 coreos.inst=yes coreos.inst.install_dev=dasda coreos.inst.image_url=ftp://
cl1.provide.example.com:8080/assets/rhcos-42.80.20191105.0-metal-dasd.raw.gz
coreos.inst.ignition_url=ftp://cl1.provide.example.com:8080/ignition-bootstrap-0
ip=172.18.78.2::172.18.78.1:255.255.255.0:::none nameserver=172.18.78.1
rd.znet=qeth,0.0.bdf0,0.0.bdf1,0.0.bdf2,layer2=1,portno=0 zfcp.allow_lun_scan=0 cio_ignore=all,
!condev rd.dasd=0.0.3490`

. Transfer the initramfs, kernel, parameter files, and RHCOS images to z/VM, for example with FTP.
. Punch the files to the virtual reader of the z/VM guest virtual machine that is to become your bootstrap node.
+
For details about the PUNCH command, see https://www.ibm.com/support/knowledgecenter/en/SSB27U_6.4.0/com.ibm.zvm.v640.dmsb4/pun.htm 
+
[TIP]
====
You can use the CP PUNCH command or, if you use Linux, the **vmur** command to transfer files between two z/VM guest virtual machines.
====
+
. Log in to CMS on the bootstrap node.
. IPL the bootstrap node from the reader: `ipl c`
. Repeat this procedure for the other machines in the cluster.
