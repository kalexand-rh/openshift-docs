// Module included in the following assemblies:
//
// * scalability_and_performance/using-argocd.adoc

[id="argocd-multi-cluster-build_{context}"]
# Managing configurations for multiple clusters with ArgoCD

You can use ArgoCD to store configurations for multiple {product-tile} clusters. In this example, you manage the Build configuration of a pre-production with the context `pre` and a production with the context `pro`. After you apply the build configurations that ArgoCD manages, you can use link:https://kustomize.io/[kustomize] to manage configuration overrides across your environments. The samples repository that you cloned includes these `kustomization` overlay files for the `pre` and `pro` clusters that you configured are included in the `./overlays` folder

.Prerequisites

* You deployed two {product-title} clusters, one to serve as a pre-production environment and one to serve as a production environment.
* You deployed an ArgoCD instance to one of the {product-title} clusters and added the samples repository to it.

.Procedure

. Create a composite `kubeconfig` file for the `pre` and `pro` clusters.
+
NOTE Setting up multiple contexts with separate kubeconfigs can be achieved by [merging kubeconfigs](https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/).

.. Obtain the `kubeconfig` files for both clusters.

.. Create a unique user in both `kubeconfig` file. For example, if each `kubeconfig` file you want to merge contains an `admin` user, then modify each `kubeconfig` file to change that user name to one that is unique in each cluster, such as `admin-pre` and `admin-pro`.

.. Merge the `kubeconfig` files:
For this example, we will have two kubeconfig files `cluster1.kubeconfig` and `cluster2.kubeconfig` that will be merged into `merged-config.kubeconfig`.
+
----
$ export KUBECONFIG="<merged.kubeconfig>:<kubeconfig-pre>:<kubeconfig-pro>" <1>
----
<1> Specify the names of the `kubeconfig` files for the `pre` and `pro` clusters and the name of the new combined `kubeconfig` file. `<merged.kubeconfig>` is the name of the new `kubeconfig` file and `<kubeconfig-pre>` and `<kubeconfig-pro>` are the `kubeconfig` files for the `pre` and `pro` clusters.

.. Confirm that you can access both clusters through the CLI:
+
----
$ oc config get-contexts
CURRENT   NAME       CLUSTER    AUTHINFO    NAMESPACE
          admin-pre  cluster1   admin-pre
          admin-pro  cluster2   admin-pro
----
+
You need the `CLUSTER` values for both clusters in the next steps.

.. Set the context for the `pre` cluster:
+
----
$ oc config set-context pre --cluster=cluster1 --user=admin-pre <1>
Context "pre" created.
----
<1> Replace `cluster1` with the name of the cluster with `admin-pre` as its name, as shown in the previous output.

.. Set the context for the `pro` cluster:
+
----
$ oc config set-context pro --cluster=cluster2 --user=admin-pro <1>
Context "pro" created.
----
<1> Replace `cluster2` with the name of the cluster with `admin-pro` as its name, as shown in the previous output.

.. Confirm that you can access the cluster that uses the `pre` context:
+
----
$ oc --context pre get nodes
NAME                           STATUS    ROLES     AGE       VERSION
ip-10-0-133-97.ec2.internal    Ready     master    5h        v1.14.6+7e13ab9a7
ip-10-0-136-91.ec2.internal    Ready     worker    5h        v1.14.6+7e13ab9a7
ip-10-0-144-237.ec2.internal   Ready     worker    5h        v1.14.6+7e13ab9a7
ip-10-0-147-216.ec2.internal   Ready     master    5h        v1.14.6+7e13ab9a7
ip-10-0-165-161.ec2.internal   Ready     master    5h        v1.14.6+7e13ab9a7
ip-10-0-169-135.ec2.internal   Ready     worker    5h        v1.14.6+7e13ab9a7
----

.. Confirm that you can access the cluster that uses the `pro` context:
+
----
$ oc --context pro get nodes
NAME                           STATUS    ROLES     AGE       VERSION
ip-10-0-133-100.ec2.internal   Ready     master    5h        v1.14.6+7e13ab9a7
ip-10-0-138-244.ec2.internal   Ready     worker    5h        v1.14.6+7e13ab9a7
ip-10-0-146-118.ec2.internal   Ready     master    5h        v1.14.6+7e13ab9a7
ip-10-0-151-40.ec2.internal    Ready     worker    5h        v1.14.6+7e13ab9a7
ip-10-0-165-83.ec2.internal    Ready     worker    5h        v1.14.6+7e13ab9a7
ip-10-0-175-20.ec2.internal    Ready     master    5h        v1.14.6+7e13ab9a7
----

. Register both clusters with the ArgoCD instance:

.. View the list of clusters that you can add to ArgoCD:
+
----
$ argocd cluster add
ERRO[0000] Choose a context name from:
CURRENT  NAME    CLUSTER    SERVER
         admin1  cluster1   https://api.cluster1.new-installer.openshift.com:6443
         admin2  cluster2   https://api.cluster2.new-installer.openshift.com:6443
*        pre     cluster1   https://api.cluster1.new-installer.openshift.com:6443
         pro     cluster2   https://api.cluster2.new-installer.openshift.com:6443
----

.. Add the cluster that uses the `pre` context to ArgoCD:
+
----
$ argocd cluster add pre
INFO[0000] ServiceAccount "argocd-manager" created in namespace "kube-system"
INFO[0000] ClusterRole "argocd-manager-role" created
INFO[0000] ClusterRoleBinding "argocd-manager-role-binding" created, bound "argocd-manager" to "argocd-manager-role"
Cluster 'pre' added
----

.. Add the cluster that uses the `pro` context to ArgoCD:
+
----
$ argocd cluster add pro
INFO[0000] ServiceAccount "argocd-manager" created in namespace "kube-system"
INFO[0000] ClusterRole "argocd-manager-role" created
INFO[0000] ClusterRoleBinding "argocd-manager-role-binding" created, bound "argocd-manager" to "argocd-manager-role"
Cluster 'pro' added
----

.. Confirm that ArgoCD recognizes both cluster instances:
+
----
$ argocd cluster list
SERVER                                                  NAME  STATUS      MESSAGE
https://kubernetes.default.svc                                Successful
https://api.cluster2.new-installer.openshift.com:6443   pro   Successful
https://api.cluster1.new-installer.openshift.com:6443   pre   Successful
----
+
If the output includes clusters that are named `pre` and `pro`, ArgoCD recognizese both clusters.

. Deploy the custom build configuration to the `pre` cluster:
+
----
$ argocd app create --project default \
                   --name pre-builds \
                   --repo https://github.com/<account_name>/openshift4-gitops.git \ <1>
                   --path builds/base \
                   --dest-server https://api.cluster1.new-installer.openshift.com:6443 \ <2>
                   --dest-namespace=openshift-config \
                   --revision master
----
<1> `<account_name>` is the name of the GitHub account or organization that you cloned the repository to.
<2> Specify the server URL that is associated with the `pre` cluster.

. Deploy the custom build configuration to the `pre` cluster:
+
----
$ argocd app create --project default \
                   --name pro-builds \
                   --repo https://github.com/<account_name>/openshift4-gitops.git \ <1>
                   --path builds/base \
                   --dest-server https://api.cluster2.new-installer.openshift.com:6443 \ <2>
                   --dest-namespace=openshift-config \
                   --revision master
----
<1> `<account_name>` is the name of the GitHub account or organization that you cloned the repository to.
<2> Specify the server URL that is associated with the `pro` cluster.

. Synchronize the configuration to both clusters.
+
----
$ argocd app sync pre-builds
$ argocd app sync pro-builds
----
+
You can define a sync policy in ArgoCD sync policy for the applications.

. Ensure that both configurations synchronized:
+
----
$ argocd app list
NAME        CLUSTER                                                 NAMESPACE         PROJECT  STATUS  HEALTH
pre-builds  https://api.cluster1.new-installer.openshift.com:6443   openshift-config  default  Synced  Healthy
pro-builds  https://api.cluster2.new-installer.openshift.com:6443   openshift-config  default  Synced  Healthy
----

. Review the build configuration for each cluster and ensure that it updated:
+
----
$ oc --context pre get build.config.openshift.io/cluster -o yaml -n openshift-config

$ oc --context pro get build.config.openshift.io/cluster -o yaml -n openshift-config
----

. Apply the `kustomize` overlays for the `pre` and `pro` clusters.

. Deploy kustomized build configuration to pre-production and production clusters,
.. Deploy the build configuration that uses the `kustomize` overlay to the `pre` cluster:
+
----
$ argocd app create --project default \
                    --name pre-kustomize-builds \
                    --repo https://github.com/<account_name>/openshift4-gitops.git \ <1>
                    --path builds/overlays/pre \
                    --dest-server https://api.cluster1.new-installer.openshift.com:6443 \ <2>
                    --dest-namespace openshift-config \
                    --revision master \
                    --sync-policy automated
----
<1> `<account_name>` is the name of the GitHub account or organization that you cloned the repository to.
<2> Specify the server URL that is associated with the `pre` cluster.

.. Deploy the build configuration that uses the `kustomize` overlay to the `pro` cluster:
+
----
$ argocd app create --project default \
                    --name pro-kustomize-builds \
                    --repo https://github.com/<account_name>/openshift4-gitops.git \ <1>
                    --path builds/overlays/pro \
                    --dest-server https://api.cluster2.new-installer.openshift.com:6443 \ <2>
                    --dest-namespace openshift-config \
                    --revision master \
                    --sync-policy automated
----
<1> `<account_name>` is the name of the GitHub account or organization that you cloned the repository to.
<2> Specify the server URL that is associated with the `pro` cluster.

.. Ensure that the new `pre` configuration application synchronized:
+
----
$ argocd app get pre-kustomize-builds
Name:               pre-kustomize-builds
Project:            default
Server:             https://api.cluster1.new-installer.openshift.com:6443
Namespace:          openshift-config
URL:                https://argocd-server-argocd.apps.cluster1.new-installer.openshift.com/applications/pre-kustomize-builds
Repo:               https://github.com/dgoodwin/openshift4-gitops.git
Target:             pre
Path:               builds/overlays/pre
Sync Policy:        Automated
Sync Status:        Synced to master (884a6db)
Health Status:      Healthy

GROUP                KIND   NAMESPACE         NAME     STATUS   HEALTH   HOOK  MESSAGE
config.openshift.io  Build  openshift-config  cluster  Running  Synced         build.config.openshift.io/cluster configured
config.openshift.io  Build                    cluster  Synced   Unknown
----

.. Ensure that the new `pro` configuration application synchronized:
+
----
$ argocd app get pro-kustomize-builds
Name:               pro-kustomize-builds
Project:            default
Server:             https://api.cluster2.new-installer.openshift.com:6443
Namespace:          openshift-config
URL:                https://argocd-server-argocd.apps.cluster2.new-installer.openshift.com/applications/pro-kustomize-builds
Repo:               https://github.com/dgoodwin/openshift4-gitops.git
Target:             pro
Path:               builds/overlays/pro
Sync Policy:        Automated
Sync Status:        Synced to master (884a6db)
Health Status:      Healthy

GROUP                KIND   NAMESPACE         NAME     STATUS   HEALTH   HOOK  MESSAGE
config.openshift.io  Build  openshift-config  cluster  Running  Synced         build.config.openshift.io/cluster unchanged
config.openshift.io  Build                    cluster  Synced   Unknown
----

.. Review the `imageLabels` for each environment and confirm that they reflect the values from the `kustomize` files,
+
----
$ oc --context pre get build.config.openshift.io/cluster -n openshift-config -o jsonpath='{.spec.buildDefaults.imageLabels}'
[map[value:true name:preprodbuild]]

$ oc --context pro get build.config.openshift.io/cluster -n openshift-config -o jsonpath='{.spec.buildDefaults.imageLabels}'
[map[value:true name:prodbuild]]
----
