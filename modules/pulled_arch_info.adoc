= Pulled procedures and examples

I converted only the conceptual information from the 3.x arch guide. That left
a variety of task information and examples that are too detailed for the spirit
of the architecture overview.

== arch-kubernetes-infrastructure

==== Mirror Pods

For example:

----
$ oc logs master-api-<hostname> -n kube-system
----

returns the standard output from the API server. However:

----
$ oc delete pod master-api-<hostname> -n kube-system
----

will not actually delete the pod.

As another example, a cluster administrator might want to perform a common
operation, such as increasing the `loglevel` of the API server to provide more
verbose data if a problem occurs. You must edit the
`/etc/origin/master/master.env` file, where the `--loglevel` parameter in the
`OPTIONS` variable can be modified, because this value is passed to the process running
inside the container. Changes require a restart of the process running inside
the container.

[discrete]
[id='control-plane-static-pods-restarting-master-services-{context}']
==== Restarting master services

To restart control plane services running in control plane static pods, use the
`master-restart` command on the master host.

To restart the master API:

----
# master-restart api
----

To restart the controllers:

----
# master-restart controllers
----

To restart etcd:

----
# master-restart etcd
----

[discrete]
[id='control-plane-static-pods-viewing-master-services-logs-{context}']
==== Viewing Master Service Logs

To view logs for control plane services running in control plane static pods,
use the `master-logs` command for the respective component:

----
# master-logs api api
# master-logs controllers controllers
# master-logs etcd etcd
----

ifdef::openshift-enterprise,openshift-origin[]
[discrete]
[id='node-bootstrapping-workflow-{context}']
==== Node Bootstrap Workflow

The process for automatic node bootstrapping uses the following workflow:

. By default during cluster installation, a set of `clusterrole`,
`clusterrolebinding` and `serviceaccount` objects are created for use in node
bootstrapping:
+
--
- The `system:node-bootstrapper` cluster role is used for creating certificate signing requests (CSRs) during node bootstrapping:
+
----
# oc describe clusterrole.authorization.openshift.io/system:node-bootstrapper

Name:			system:node-bootstrapper
Created:		17 hours ago
Labels:			kubernetes.io/bootstrapping=rbac-defaults
Annotations:		authorization.openshift.io/system-only=true
			openshift.io/reconcile-protect=false
Verbs			Non-Resource URLs	Resource Names	API Groups		Resources
[create get list watch]	[]			[]		[certificates.k8s.io]	[certificatesigningrequests]
----

- The following `node-bootstrapper` service account is created in the
`openshift-infra` project:
+
----
# oc describe sa node-bootstrapper -n openshift-infra

Name:                node-bootstrapper
Namespace:           openshift-infra
Labels:              <none>
Annotations:         <none>
Image pull secrets:  node-bootstrapper-dockercfg-f2n8r
Mountable secrets:   node-bootstrapper-token-79htp
                     node-bootstrapper-dockercfg-f2n8r
Tokens:              node-bootstrapper-token-79htp
                     node-bootstrapper-token-mqn2q
Events:              <none>
----

- The following `system:node-bootstrapper` cluster role binding is for the node
bootstrapper cluster role and service account:
+
----
# oc describe clusterrolebindings system:node-bootstrapper

Name:			system:node-bootstrapper
Created:		17 hours ago
Labels:			<none>
Annotations:		openshift.io/reconcile-protect=false
Role:			/system:node-bootstrapper
Users:			<none>
Groups:			<none>
ServiceAccounts:	openshift-infra/node-bootstrapper
Subjects:		<none>
Verbs			Non-Resource URLs	Resource Names	API Groups		Resources
[create get list watch]	[]			[]		[certificates.k8s.io]	[certificatesigningrequests]
----
--

. Also by default during cluster installation, the `openshift-ansible` installer creates a
{product-title} certificate authority and various other certificates, keys, and
`kubeconfig` files in the `/etc/origin/master` directory. Two files of note
are:
+
--
[horizontal]
`/etc/origin/master/admin.kubeconfig`:: Uses the `system:admin` user.
`/etc/origin/master/bootstrap.kubeconfig`:: Used for node bootstrapping nodes other than masters.
--

.. The `etc/origin/master/bootstrap.kubeconfig` is created when the installer
uses the `node-bootstrapper` service account as follows:
+
----
$ oc --config=/etc/origin/master/admin.kubeconfig \
    serviceaccounts create-kubeconfig node-bootstrapper \
    -n openshift-infra
----

.. On master nodes, the `/etc/origin/master/admin.kubeconfig` is used as a
bootstrapping file and is copied to `/etc/origin/node/boostrap.kubeconfig`. On
other, non-master nodes, the `/etc/origin/master/bootstrap.kubeconfig` file is
copied to all other nodes in at `/etc/origin/node/boostrap.kubeconfig` on each
node host.

.. The `/etc/origin/master/bootstrap.kubeconfig` is then passed to kubelet using
the flag `--bootstrap-kubeconfig` as follows:
+
----
--bootstrap-kubeconfig=/etc/origin/node/bootstrap.kubeconfig
----

. The kubelet is first started with the supplied
`/etc/origin/node/bootstrap.kubeconfig` file. After initial connection
internally, the kubelet creates certificate signing requests (CSRs) and sends
them to the master.

. The CSRs are verified and approved via the controller manager (specifically the
certificate signing controller). If approved, the kubelet client and server
certificates are created in the `/etc/origin/node/ceritificates` directory.
For example:
+
----
# ls -al /etc/origin/node/certificates/
total 12
drwxr-xr-x. 2 root root  212 Jun 18 21:56 .
drwx------. 4 root root  213 Jun 19 15:18 ..
-rw-------. 1 root root 2826 Jun 18 21:53 kubelet-client-2018-06-18-21-53-15.pem
-rw-------. 1 root root 1167 Jun 18 21:53 kubelet-client-2018-06-18-21-53-45.pem
lrwxrwxrwx. 1 root root   68 Jun 18 21:53 kubelet-client-current.pem -> /etc/origin/node/certificates/kubelet-client-2018-06-18-21-53-45.pem
-rw-------. 1 root root 1447 Jun 18 21:56 kubelet-server-2018-06-18-21-56-52.pem
lrwxrwxrwx. 1 root root   68 Jun 18 21:56 kubelet-server-current.pem -> /etc/origin/node/certificates/kubelet-server-2018-06-18-21-56-52.pem
----

. After the CSR approval, the `node.kubeconfig` file is created at
`/etc/origin/node/node.kubeconfig`.

. The kubelet is restarted with the `/etc/origin/node/node.kubeconfig` file and
the certificates in the `/etc/origin/node/certificates/` directory, after
which point it is ready to join the cluster.

[discrete]
[id='node-bootstrapping-configuration-workflow-{context}']
==== Node Configuration Workflow

Sourcing a node's configuration uses the following workflow:

. Initially the node's kubelet is started with the bootstrap configuration file,
`bootstrap-node-config.yaml` in the `/etc/origin/node/` directory, created
at the time of node provisioning.

. On each node, the node service file uses the local script
`openshift-node` in the `/usr/local/bin/` directory to start the kubelet
with the supplied `bootstrap-node-config.yaml`.

. On each master, the directory `/etc/origin/node/pods` contains pod manifests
for `apiserver`, `controller` and `etcd` which are created as static pods on
masters.

. During cluster installation, a sync DaemonSet is created which creates a sync
pod on each node. The sync pod monitors changes in the file
`/etc/sysconfig/atomic-openshift-node`. It specifically watches for
`BOOTSTRAP_CONFIG_NAME` to be set. `BOOTSTRAP_CONFIG_NAME` is set by the
`openshift-ansible` installer and is the name of the ConfigMap based on the node
configuration group the node belongs to.
+
By default, the installer creates the following node configuration groups:
+
--
- `node-config-master`
- `node-config-infra`
- `node-config-compute`
- `node-config-all-in-one`
- `node-config-master-infra`
--
+
A ConfigMap for each group is created in the `openshift-node` project.

. The sync pod extracts the appropriate ConfigMap based on the value set in
`BOOTSTRAP_CONFIG_NAME`.

. The sync pod converts the ConfigMap data into kubelet configurations and creates
a `/etc/origin/node/node-config.yaml` for that node host. If a change is made
to this file (or it is the file's initial creation), the kubelet is restarted.

[discrete]
[id='node-bootstrapping-modifying-configurations-{context}']
==== Modifying Node Configurations

A node's configuration is modified by editing the appropriate ConfigMap in the
`openshift-node` project. The `/etc/origin/node/node-config.yaml` must not be
modified directly.

For example, for a node that is in the `node-config-compute` group, edit the
ConfigMap using:

----
$ oc edit cm node-config-compute -n openshift-node
----

endif::[]

== arch-routes

=== Load-balancing Strategy

The `ROUTER_TCP_BALANCE_SCHEME` environment variable sets the default
strategy for passthorugh routes. The `ROUTER_LOAD_BALANCE_ALGORITHM` environment
variable sets the default strategy for the router for the remaining routes.
A route specific annotation,
`haproxy.router.openshift.io/balance`, can be used to control specific routes.

.HAProxy Strict SNI


The option can be set when the router is created or added later.

----
$ oc adm router --strict-sni
----

This sets `ROUTER_STRICT_SNI=true`.
endif::[]

== Router cipher suite

Each client (for example, Chrome 30, or Java8) includes a suite of ciphers used
to securely connect with the router. The router must have at least one of the
ciphers for the connection to be complete:

.Router cipher profiles
[cols="2,6", options="header"]
|===
|Profile | Oldest compatible client
|modern| Firefox 27, Chrome 30, IE 11 on Windows 7, Edge, Opera 17, Safari 9, Android 5.0, Java 8
|intermediate|Firefox 1, Chrome 1, IE 7, Opera 5, Safari 1, Windows XP IE8, Android 2.3, Java 7
|old|Windows XP IE6, Java 6
|===

See the link:https://wiki.mozilla.org/Security/Server_Side_TLS[Security/Server Side TLS]
reference guide for more information.

The router defaults to the `intermediate` profile. You can select a different
profile using the `--ciphers` option when creating a route, or by changing
the `ROUTER_CIPHERS` environment variable with the values `modern`,
`intermediate`, or `old` for an existing router. Alternatively, a set of ":"
separated ciphers can be provided. The ciphers must be from the set displayed
by:

----
openssl ciphers
----
endif::openshift-origin,openshift-enterprise[]

== Route host names

.A Route with a specified host:

[source,yaml]
----
apiVersion: v1
kind: Route
metadata:
  name: host-route
spec:
  host: www.example.com  <1>
  to:
    kind: Service
    name: service-name
----
<1> Specifies the externally-reachable host name used to expose a service.

.A Route Without a Host:

[source,yaml]
----
apiVersion: v1
kind: Route
metadata:
  name: no-route-hostname
spec:
  to:
    kind: Service
    name: service-name
----

If a host name is not provided as part of the route definition, then
{product-title} automatically generates one for you. The generated host name
is of the form:

----
<route-name>[-<namespace>].<suffix>
----

The following example shows the {product-title}-generated host name for the
above configuration of a route without a host added to a namespace
`mynamespace`:

.Generated host name
----
no-route-hostname-mynamespace.router.default.svc.cluster.local <1>
----
<1> The generated host name suffix is the default routing subdomain
`router.default.svc.cluster.local`.

A cluster administrator can also customize the suffix used as the default
routing subdomain for their environment.


.Unsecured Route Object YAML Definition

[source,yaml]
----
apiVersion: v1
kind: Route
metadata:
  name: route-unsecured
spec:
  host: www.example.com
  to:
    kind: Service
    name: service-name
----


.An unsecured route with a path:

[source,yaml]
----
apiVersion: v1
kind: Route
metadata:
  name: route-unsecured
spec:
  host: www.example.com
  path: "/test"   <1>
  to:
    kind: Service
    name: service-name
----
<1> The path is the only added attribute for a path-based route.


.An unsecured route with a path:

[source,yaml]
----
apiVersion: v1
kind: Route
metadata:
  name: route-unsecured
spec:
  host: www.example.com
  path: "/test"   <1>
  to:
    kind: Service
    name: service-name
----
<1> The path is the only added attribute for a path-based route.

Edge-terminated routes can specify an `insecureEdgeTerminationPolicy` that
enables traffic on insecure schemes (`HTTP`) to be disabled, allowed or
redirected.
The allowed values for `insecureEdgeTerminationPolicy` are:
  `None` or empty (for disabled), `Allow` or `Redirect`.
The default `insecureEdgeTerminationPolicy` is to disable traffic on the
insecure scheme. A common use case is to allow content to be served via a
secure scheme but serve the assets (example images, stylesheets and
javascript) via the insecure scheme.


.An unsecured route with a path:

[source,yaml]
----
apiVersion: v1
kind: Route
metadata:
  name: route-unsecured
spec:
  host: www.example.com
  path: "/test"   <1>
  to:
    kind: Service
    name: service-name
----
<1> The path is the only added attribute for a path-based route.

.A Secured Route Using Edge Termination Allowing HTTP Traffic


[source,yaml]
----
apiVersion: v1
kind: Route
metadata:
  name: route-edge-secured-allow-insecure <1>
spec:
  host: www.example.com
  to:
    kind: Service
    name: service-name <1>
  tls:
    termination:                   edge   <2>
    insecureEdgeTerminationPolicy: Allow  <3>
    [ ... ]
----
<1> The name of the object, which is limited to 63 characters.
<2> The `termination` field is `edge` for edge termination.
<3> The insecure policy to allow requests sent on an insecure scheme `HTTP`.


.A Secured Route Using Edge Termination Redirecting HTTP Traffic to HTTPS


[source,yaml]
----
apiVersion: v1
kind: Route
metadata:
  name: route-edge-secured-redirect-insecure <1>
spec:
  host: www.example.com
  to:
    kind: Service
    name: service-name <1>
  tls:
    termination:                   edge      <2>
    insecureEdgeTerminationPolicy: Redirect  <3>
    [ ... ]
----
<1> The name of the object, which is limited to 63 characters.
<2> The `termination` field is `edge` for edge termination.
<3> The insecure policy to redirect requests sent on an insecure scheme `HTTP` to a secure scheme `HTTPS`.

.A Secured Route Using Passthrough Termination

[source,yaml]
----
apiVersion: v1
kind: Route
metadata:
  name: route-passthrough-secured <1>
spec:
  host: www.example.com
  to:
    kind: Service
    name: service-name <1>
  tls:
    termination: passthrough     <2>
----
<1> The name of the object, which is limited to 63 characters.
<2> The `termination` field is set to `passthrough`. No other encryption fields are needed.



.A Secured Route Using Re-Encrypt Termination


[source,yaml]
----
apiVersion: v1
kind: Route
metadata:
  name: route-pt-secured <1>
spec:
  host: www.example.com
  to:
    kind: Service
    name: service-name <1>
  tls:
    termination: reencrypt        <2>
    key: [as in edge termination]
    certificate: [as in edge termination]
    caCertificate: [as in edge termination]
    destinationCACertificate: |-  <3>
      -----BEGIN CERTIFICATE-----
      [...]
      -----END CERTIFICATE-----
----

<1> The name of the object, which is limited to 63 characters.
<2> The `termination` field is set to `reencrypt`. Other fields are as in edge
termination.
<3> Required for re-encryption. `destinationCACertificate`
specifies a CA certificate to validate the endpoint certificate, securing the
connection from the router to the destination pods. If the service is using a service signing certificate, or the administrator has specified a default CA certificate for the router and the service has a certificate signed by that CA, this field can be omitted.

[[route-specific-annotations]]
== Route-specific Annotations

Using environment variables, a router can set the default
options for all the routes it exposes. An individual route can override some
of these defaults by providing specific configurations in its annotations.

.Route Annotations

For all the items outlined in this section, you can set annotations on the
`route definition` for the route to alter its configuration

.Route Annotations
[cols="3*", options="header"]
|===
|Variable | Description | Environment Variable Used as Default
|`haproxy.router.openshift.io/balance`| Sets the load-balancing algorithm. Available options are `source`, `roundrobin`, and `leastconn`. | `ROUTER_TCP_BALANCE_SCHEME` for passthrough routes. Otherwise, use `ROUTER_LOAD_BALANCE_ALGORITHM`.
|`haproxy.router.openshift.io/disable_cookies`| Disables the use of cookies to track related connections. If set to `true` or `TRUE`, the balance algorithm is used to choose which back-end serves connections for each incoming HTTP request. |
|`router.openshift.io/cookie_name`| Specifies an optional cookie to use for
this route. The name must consist of any combination of upper and lower case letters, digits, "_",
and "-". The default is the hashed internal key name for the route. |
|`haproxy.router.openshift.io/pod-concurrent-connections`| Sets the maximum number of connections that are allowed to a backing pod from a router.  Note: if there are multiple pods, each can have this many connections.  But if you have multiple routers, there is no coordination among them, each may connect this many times. If not set, or set to 0, there is no limit. |
|`haproxy.router.openshift.io/rate-limit-connections`| Setting `true` or `TRUE` to enables rate limiting functionality. |
|`haproxy.router.openshift.io/rate-limit-connections.concurrent-tcp`| Limits the number of concurrent TCP connections shared by an IP address. |
|`haproxy.router.openshift.io/rate-limit-connections.rate-http`| Limits the rate at which an IP address can make HTTP requests. |
|`haproxy.router.openshift.io/rate-limit-connections.rate-tcp`| Limits the rate at which an IP address can make TCP connections. |
|`haproxy.router.openshift.io/timeout` | Sets a server-side timeout for the route. (TimeUnits) | `ROUTER_DEFAULT_SERVER_TIMEOUT`
|`router.openshift.io/haproxy.health.check.interval`| Sets the interval for the back-end health checks. (TimeUnits) | `ROUTER_BACKEND_CHECK_INTERVAL`
|`haproxy.router.openshift.io/ip_whitelist` | Sets a xref:whitelist[whitelist] for the route. |
|`haproxy.router.openshift.io/hsts_header` | Sets a Strict-Transport-Security header for the edge terminated or re-encrypt route. |
|===

.A Route Setting Custom Timeout

[source,yaml]
----
apiVersion: v1
kind: Route
metadata:
  annotations:
    haproxy.router.openshift.io/timeout: 5500ms <1>
[...]
----
<1> Specifies the new timeout with HAProxy supported units (us, ms, s, m, h, d).
If unit not provided, ms is the default.


[NOTE]
====
Setting a server-side timeout value for passthrough routes too low can cause
WebSocket connections to timeout frequently on that route.
====

[[whitelist]]
== Route-specific IP Whitelists

You can restrict access to a route to a select set of IP addresses by adding the
`haproxy.router.openshift.io/ip_whitelist` annotation on the route. The
whitelist is a space-separated list of IP addresses and/or CIDRs for the
approved source addresses. Requests from IP addresses that are not in the
whitelist are dropped.

Some examples:

When editing a route, add the following annotation to define the desired
source IP's. Alternatively, use `oc annotate route <name>`.

Allow only one specific IP address:

----
metadata:
  annotations:
    haproxy.router.openshift.io/ip_whitelist: 192.168.1.10
----

Allow several IP addresses:

----
metadata:
  annotations:
    haproxy.router.openshift.io/ip_whitelist: 192.168.1.10 192.168.1.11 192.168.1.12
----

Allow an IP CIDR network:

----
metadata:
  annotations:
    haproxy.router.openshift.io/ip_whitelist: 192.168.1.0/24
----

Allow mixed IP addresses and IP CIDR networks:

----
metadata:
  annotations:
    haproxy.router.openshift.io/ip_whitelist: 180.5.61.153 192.168.1.0/24 10.0.0.0/8
----
endif::openshift-origin,openshift-enterprise,openshift-dedicated[]

ifdef::openshift-origin,openshift-enterprise[]
[[wildcard-subdomain-route-policy]]
== Creating Routes Specifying a Wildcard Subdomain Policy

A wildcard policy allows a user to define a route that covers all hosts within a
domain (when the router is configured to allow it). A route can specify a
wildcard policy as part of its configuration using the `wildcardPolicy` field.
Any routers run with a policy allowing wildcard routes will expose the route
appropriately based on the wildcard policy.

xref:../../install_config/router/default_haproxy_router.adoc#using-wildcard-routes[Learn how to configure HAProxy routers to allow wildcard routes].


.A Route Specifying a Subdomain WildcardPolicy

[source,yaml]
----
apiVersion: v1
kind: Route
spec:
  host: wildcard.example.com  <1>
  wildcardPolicy: Subdomain   <2>
  to:
    kind: Service
    name: service-name
----
<1> Specifies the externally reachable host name used to expose a service.
<2> Specifies that the externally reachable host name should allow all hosts
    in the subdomain `example.com`. `.example.com` is the subdomain for host
    name `wildcard.example.com` to reach the exposed service.


[[route-status-field]]
== Route Status

The `route status` field is only set by routers. If changes are made to a route
so that a router no longer serves a specific route, the status becomes stale.
The routers do not clear the `route status` field. To remove the stale entries
in the route status, use the
link:https://github.com/openshift/origin/blob/master/images/router/clear-route-status.sh[clear-route-status
script].

[[architecture-core-concepts-routes-deny-allow]]
== Denying or Allowing Certain Domains in Routes

A router can be configured to deny or allow a specific subset of domains from
the host names in a route using the `ROUTER_DENIED_DOMAINS` and
`ROUTER_ALLOWED_DOMAINS` environment variables.

[cols="2"]
|===

|`ROUTER_DENIED_DOMAINS` | Domains listed are not allowed in any indicated routes.
|`ROUTER_ALLOWED_DOMAINS` | Only the domains listed are allowed in any indicated routes.

|===

The domains in the list of denied domains take precedence over the list of
allowed domains. Meaning {product-title} first checks the deny list (if
applicable), and if the host name is not in the list of denied domains, it then
checks the list of allowed domains. However, the list of allowed domains is more
restrictive, and ensures that the router only admits routes with hosts that
belong to that list.

For example, to deny the `[{asterisk}.]open.header.test`, `[{asterisk}.]openshift.org` and
`[{asterisk}.]block.it` routes for the `myrouter` route:

----
$ oc adm router myrouter ...
$ oc set env dc/myrouter ROUTER_DENIED_DOMAINS="open.header.test, openshift.org, block.it"
----

This means that `myrouter` will admit the following based on the route's name:

----
$ oc expose service/<name> --hostname="foo.header.test"
$ oc expose service/<name> --hostname="www.allow.it"
$ oc expose service/<name> --hostname="www.openshift.test"
----

However, `myrouter` will deny the following:

----
$ oc expose service/<name> --hostname="open.header.test"
$ oc expose service/<name> --hostname="www.open.header.test"
$ oc expose service/<name> --hostname="block.it"
$ oc expose service/<name> --hostname="franco.baresi.block.it"
$ oc expose service/<name> --hostname="openshift.org"
$ oc expose service/<name> --hostname="api.openshift.org"
----

Alternatively, to block any routes where the host name is _not_ set to `[{asterisk}.]stickshift.org` or `[{asterisk}.]kates.net`:

----
$ oc adm router myrouter ...
$ oc set env dc/myrouter ROUTER_ALLOWED_DOMAINS="stickshift.org, kates.net"
----

This means that the `myrouter` router will admit:

----
$ oc expose service/<name> --hostname="stickshift.org"
$ oc expose service/<name> --hostname="www.stickshift.org"
$ oc expose service/<name> --hostname="kates.net"
$ oc expose service/<name> --hostname="api.kates.net"
$ oc expose service/<name> --hostname="erno.r.kube.kates.net"
----

However, `myrouter` will deny the following:

----
$ oc expose service/<name> --hostname="www.open.header.test"
$ oc expose service/<name> --hostname="drive.ottomatic.org"
$ oc expose service/<name> --hostname="www.wayless.com"
$ oc expose service/<name> --hostname="www.deny.it"
----

To implement both scenarios, run:

----
$ oc adm router adrouter ...
$ oc set env dc/adrouter ROUTER_ALLOWED_DOMAINS="okd.io, kates.net" \
    ROUTER_DENIED_DOMAINS="ops.openshift.org, metrics.kates.net"
----

This will allow any routes where the host name is set to `[{asterisk}.]openshift.org` or
`[{asterisk}.]kates.net`, and not allow any routes where the host name is set to
`[{asterisk}.]ops.openshift.org` or `[{asterisk}.]metrics.kates.net`.

Therefore, the following will be denied:

----
$ oc expose service/<name> --hostname="www.open.header.test"
$ oc expose service/<name> --hostname="ops.openshift.org"
$ oc expose service/<name> --hostname="log.ops.openshift.org"
$ oc expose service/<name> --hostname="www.block.it"
$ oc expose service/<name> --hostname="metrics.kates.net"
$ oc expose service/<name> --hostname="int.metrics.kates.net"
----

However, the following will be allowed:

----
$ oc expose service/<name> --hostname="openshift.org"
$ oc expose service/<name> --hostname="api.openshift.org"
$ oc expose service/<name> --hostname="m.api.openshift.org"
$ oc expose service/<name> --hostname="kates.net"
$ oc expose service/<name> --hostname="api.kates.net"
----

[[disable-namespace-ownership-check]]
== Disabling the Namespace Ownership Check

Hosts and subdomains are owned by the namespace of the route that first
makes the claim. Other routes created in the namespace can make claims on
the subdomain. All other namespaces are prevented from making claims on
the claimed hosts and subdomains. The namespace that owns the host also
owns all paths associated with the host, for example `_www.abc.xyz/path1_`.

For example, if the host `_www.abc.xyz_` is not claimed by any route.
Creating route `r1` with host `_www.abc.xyz_` in namespace `ns1` makes
namespace `ns1` the owner of host `_www.abc.xyz_` and subdomain `abc.xyz`
for wildcard routes. If another namespace, `ns2`, tries to create a route
with say a different path `_www.abc.xyz/path1/path2_`, it would fail
because a route in another namespace (`ns1` in this case) owns that host.

With
xref:../../install_config/router/default_haproxy_router.adoc#using-wildcard-routes[wildcard routes]
the namespace that owns the subdomain owns all hosts in the subdomain.
If a namespace owns subdomain `abc.xyz` as in the above example,
another namespace cannot claim `z.abc.xyz`.

By disabling the namespace ownership rules, you can disable these restrictions
and allow hosts (and subdomains) to be claimed across namespaces.

[WARNING]
====
If you decide to disable the namespace ownership checks in your router,
be aware that this allows end users to claim ownership of hosts
across namespaces. While this change can be desirable in certain
development environments, use this feature with caution in production
environments, and ensure that your cluster policy has locked down untrusted end
users from creating routes.
====

For example, with `ROUTER_DISABLE_NAMESPACE_OWNERSHIP_CHECK=true`, if
namespace `ns1` creates the oldest route `r1`  `_www.abc.xyz_`,  it owns only
the hostname (+ path).  Another namespace can create a wildcard route
even though it does not have the oldest route in that subdomain (`abc.xyz`)
and we could potentially have other namespaces claiming other
non-wildcard overlapping hosts (for example, `foo.abc.xyz`, `bar.abc.xyz`,
`baz.abc.xyz`) and their claims would be granted.

Any other namespace (for example, `ns2`) can now create
a route `r2`  `_www.abc.xyz/p1/p2_`,  and it would be admitted.  Similarly
another namespace (`ns3`) can also create a route  `wildthing.abc.xyz`
with a subdomain wildcard policy and it can own the wildcard.

As this example demonstrates, the policy `ROUTER_DISABLE_NAMESPACE_OWNERSHIP_CHECK=true` is more
lax and allows claims across namespaces.  The only time the router would
reject a route with the namespace ownership disabled is if the host+path
is already claimed.

For example, if a new route `rx` tries to claim `_www.abc.xyz/p1/p2_`, it
would be rejected as route `r2` owns that host+path combination.  This is true whether route `rx`
is in the same namespace or other namespace since the exact host+path is already claimed.

This feature can be set during router creation or by setting an environment
variable in the router's deployment configuration.

----
$ oc adm router ... --disable-namespace-ownership-check=true
----

----
$ oc set env dc/router ROUTER_DISABLE_NAMESPACE_OWNERSHIP_CHECK=true
----
endif::openshift-origin,openshift-enterprise[]

For example, an ingress object configured as:

[source, yaml]
----
kind: Ingress
apiVersion: extensions/v1beta1
metadata:
  name: test
spec:
  rules:
  - host: test.com
    http:
     paths:
     - path: /test
       backend:
        serviceName: test-1
        servicePort: 80
----

generates the following route object:

[source, yaml]
----
kind: Route
apiVersion: route.openshift.io/v1
metadata:
  name: test-a34th <1>
  ownerReferences:
  - apiVersion: extensions/v1beta1
    kind: Ingress
    name: test
    controller: true
spec:
  host: test.com
  path: /test
  to:
    name: test-1
  port:
     targetPort: 80
----
<1> The name is generated by the route objects, with the ingress name as a prefix.

[NOTE]
====
In order for a route to be created, an ingress object must have a host,
service, and path.
====

== Alternate backend weights

The following is an example route configuration using alternate backends for
xref:../../dev_guide/deployments/advanced_deployment_strategies.adoc#advanced-deployment-a-b-deployment[A/B
deployments].

.A Route with alternateBackends and weights:

[source,yaml]
----
apiVersion: v1
kind: Route
metadata:
  name: route-alternate-service
  annotations:
    haproxy.router.openshift.io/balance: roundrobin  <1>
spec:
  host: www.example.com
  to:
    kind: Service
    name: service-name  <2>
    weight: 20          <4>
  alternateBackends:
  - kind: Service
    name: service-name2 <3>
    weight: 10          <4>
    kind: Service
    name: service-name3 <3>
    weight: 10          <4>
----

<1> This route uses `roundrobin`
ifdef::openshift-origin,openshift-enterprise[]
xref:load-balancing[load balancing strategy].
endif::[]
ifdef::openshift-dedicated[]
load balancing strategy
endif::[]
<2> The first service name is `service-name` which may have 0 or more pods
<3> The alternateBackend services may also have 0 or more pods
<4> The total `weight` is 40. `service-name` will get 20/40 or 1/2 of the requests,
`service-name2` and `service-name3` will each get 1/4 of the requests, assuming each
service has 1 or more endpoints.

== Services

The following
example shows the definition of a service for the pod defined above:

.Service Object Definition (YAML)
====

[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
 name: docker-registry      <1>
spec:
 selector:                  <2>
	 docker-registry: default
 clusterIP: 172.30.136.123   <3>
 ports:
 - nodePort: 0
	 port: 5000               <4>
	 protocol: TCP
	 targetPort: 5000         <5>
----

<1> The service name *docker-registry* is also used to construct an
environment variable with the service IP that is inserted into other
pods in the same namespace. The maximum name length is 63 characters.
<2> The label selector identifies all pods with the
*docker-registry=default* label attached as its backing pods.
<3> Virtual IP of the service, allocated automatically at creation from a pool
of internal IPs.
<4> Port the service listens on.
<5> Port on the backing pods to which the service forwards connections.
====


The externalIPs must be selected by the cluster adminitrators from the
*externalIPNetworkCIDRs* range configured in
xref:../../admin_guide/tcp_ingress_external_ports.adoc#unique-external-ips-ingress-traffic-configure-cluster[*_master-config.yaml_*]
file. When *_master-config.yaml_* is changed, the master services must be
restarted.

.Sample externalIPNetworkCIDR /etc/origin/master/master-config.yaml
====
----
networkConfig:
  externalIPNetworkCIDRs:
  - 192.0.1.0.0/24
----
====

.Service externalIPs Definition (JSON)
====

[source,json]
----
{
    "kind": "Service",
    "apiVersion": "v1",
    "metadata": {
        "name": "my-service"
    },
    "spec": {
        "selector": {
            "app": "MyApp"
        },
        "ports": [
            {
                "name": "http",
                "protocol": "TCP",
                "port": 80,
                "targetPort": 9376
            }
        ],
        "externalIPs" : [
            "192.0.1.1"         <1>
        ]
    }
}
----

<1> List of external IP addresses on which the *port* is exposed. This list is in addition to the internal IP address list.

====


The pool is configured in *_/etc/origin/master/master-config.yaml_* file. After
changing this file, restart the master service.

The `ingressIPNetworkCIDR` is set to `172.29.0.0/16` by default. If the cluster
environment is not already using this private range, use the default range or
set a custom range.

[NOTE]
====
If you are using xref:../../admin_guide/high_availability.adoc#admin-guide-high-availability[high availability], then this range must be less than 256
addresses.
====

.Sample ingressIPNetworkCIDR /etc/origin/master/master-config.yaml
====
----
networkConfig:
  ingressIPNetworkCIDR: 172.29.0.0/16
----
====

endif::[]

ifdef::openshift-origin,openshift-enterprise[]
[[service-nodeport]]
== Service NodePort

Setting the service `type=NodePort` will allocate a port from a flag-configured range (default: 30000-32767), and each node will proxy that port (the same port number on every node) into your service.

The selected port will be reported in the service configuration, under  `spec.ports[*].nodePort`.

To specify a custom port just place the port number in the nodePort field. The custom port number must be in the configured range for nodePorts. When '*master-config.yaml*' is changed the master services must be restarted.

.Sample servicesNodePortRange /etc/origin/master/master-config.yaml
====
----
kubernetesMasterConfig:
  servicesNodePortRange: ""
----
====

The service will be visible as both the `<NodeIP>:spec.ports[].nodePort`
and `spec.clusterIp:spec.ports[].port`

[NOTE]
====
Setting a nodePort is a privileged operation.
====
endif::[]


[[headless-service-creation]]
=== Creating a headless service
Creating a headless service is similar to creating a standard service, but you
do not declare the `ClusterIP` address. To create a headless service, add the
`clusterIP: None` parameter value to the service YAML definition.

For example, for a group of pods that you want to be a part of the same cluster or service.

.List of pods
[source, bash]
----
$ oc get pods -o wide
NAME               READY  STATUS    RESTARTS   AGE    IP            NODE
frontend-1-287hw   1/1    Running   0          7m     172.17.0.3    node_1
frontend-1-68km5   1/1    Running   0          7m     172.17.0.6    node_1
----

You can define the headless service as:

.Headless service definition
[source, yaml]
----
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ruby-helloworld-sample
    template: application-template-stibuild
  name: frontend-headless <1>
spec:
  clusterIP: None <2>
  ports:
  - name: web
    port: 5432
    protocol: TCP
    targetPort: 8080
  selector:
    name: frontend <3>
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
----

<1> Name of the headless service.
<2> Setting `clusterIP` variable to `None` declares a headless service.
<3> Selects all pods that have `frontend` label.

Also, headless service does not have any IP address of its own.

[source, bash]
----
$ oc get svc
NAME                TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
frontend            ClusterIP   172.30.232.77    <none>        5432/TCP   12m
frontend-headless   ClusterIP   None             <none>        5432/TCP   10m
----


[[headless-service-endpoint-discovery]]
=== Endpoint discovery by using a headless service


When you look up the DNS `A` record for a standard service, you get the loadbalanced IP of the service.

[source, bash]
----
$ dig frontend.test A +search +short
172.30.232.77
----

But for a headless service, you get the list of IPs of individual pods.
[source, bash]
----
$ dig frontend-headless.test A +search +short
172.17.0.3
172.17.0.6
----

[NOTE]
====
For using a headless service with a StatefulSet and related use cases where you
need to resolve DNS for the pod during initialization and termination, set
`publishNotReadyAddresses` to `true` (the default value is `false`). When
`publishNotReadyAddresses` is set to `true`, it indicates that DNS
implementations must publish the `notReadyAddresses` of subsets for the
Endpoints associated with the Service.
====

== Admission control

=== Webhooks

The communication between the admission controller and the webhook server needs to be
secured using  TLS. Generate a CA certificate and use the certificate to sign the server certificate
used by your webhook server. The PEM-formatted CA certificate is supplied
to the admission controller using a mechanism, such as
xref:../../dev_guide/secrets.adoc#service-serving-certificate-secrets[Service Serving Certificate Secrets].


[[architecture-additional-concepts-dynamic-admission-webhooks-ex-m]]
.Sample mutating admission webhook configuration:

[source,yaml]
----
apiVersion: admissionregistration.k8s.io/v1beta1
  kind: MutatingWebhookConfiguration <1>
  metadata:
    name: <controller_name> <2>
  webhooks:
  - name: <webhook_name> <3>
    clientConfig: <4>
      service:
        namespace:  <5>
        name: <6>
       path: <webhook_url> <7>
      caBundle: <cert> <8>
    rules: <9>
    - operations: <10>
      - <operation>
      apiGroups:
      - ""
      apiVersions:
      - "*"
      resources:
      - <resource>
    failurePolicy: <policy> <11>
----

<1> Specifies a mutating admission webhook configuration.
<2> The name for the admission webhook object.
<3> The name of the webhook to call.
<4> Information about how to connect to, trust, and send data to the webhook server.
<5> The project where the front-end service is created.
<6> The name of the front-end service.
<7> The webhook URL used for admission requests.
<8> A PEM-encoded CA certificate that signs the server certificate used by the webhook server.
<9> Rules that define when the API server should use this controller.
<10> The operation(s) that triggers the API server to call this controller:
* create
* update
* delete
* connect
<11> Specifies how the policy should proceed if the webhook admission server is unavailable.
Either `Ignore` (allow/fail open) or `Fail` (block/fail closed).

[[architecture-additional-concepts-dynamic-admission-webhooks-ex-v]]
//http://blog.kubernetes.io/2018/01/extensible-admission-is-beta.html
.Sample validating admission webhook configuration:

[source,yaml]
----
apiVersion: admissionregistration.k8s.io/v1beta1
  kind: ValidatingWebhookConfiguration <1>
  metadata:
    name: <controller_name> <2>
  webhooks:
  - name: <webhook_name> <3>
    clientConfig: <4>
      service:
        namespace: default  <5>
        name: kubernetes <6>
       path: <webhook_url> <7>
      caBundle: <cert> <8>
    rules: <9>
    - operations: <10>
      - <operation>
      apiGroups:
      - ""
      apiVersions:
      - "*"
      resources:
      - <resource>
    failurePolicy: <policy> <11>
----

<1> Specifies a validating admission webhook configuration.
<2> The name for the webhook admission object.
<3> The name of the webhook to call.
<4> Information about how to connect to, trust, and send data to the webhook server.
<5> The project where the front-end service is created.
<6> The name of the front-end service.
<7> The webhook URL used for admission requests.
<8> A PEM-encoded CA certificate that signs the server certificate used by the webhook server.
<9> Rules that define when the API server should use this controller.
<10> The operation that triggers the API server to call this controller.
* create
* update
* delete
* connect
<11> Specifies how the policy should proceed if the webhook admission server is unavailable.
Either `Ignore` (allow/fail open) or `Fail` (block/fail closed).

[NOTE]
====
Fail open can result in unpredictable behavior for all clients.
====



[[architecture-additional-concepts-dynamic-admission-webhooks-writing]]
=== Create the Admission Webhook

First deploy the external webhook server and ensure
it is working properly. Otherwise, depending whether the webhook is configured as `fail open` or
`fail closed`, operations will be unconditionally accepted or rejected.

. Configure a xref:architecture-additional-concepts-dynamic-admission-webhooks-ex-m[mutating]
or xref:architecture-additional-concepts-dynamic-admission-webhooks-ex-v[validating] admission webhook object in a YAML file.

. Run the following command to create the object:
+
----
oc create -f <file-name>.yaml
----
+
After you create the admission webhook object, {product-title} takes a few
seconds to honor the new configuration.

. Create a front-end service for the admission webhook:
+
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  labels:
    role: webhook <1>
  name: <name>
spec:
  selector:
   role: webhook <1>
----
+
<1> Free-form label to trigger the webhook.

. Run the following command to create the object:
+
----
oc create -f <file-name>.yaml
----

. Add the admission webhook name to pods you want controlled by the webhook:
+
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  labels:
    role: webhook <1>
  name: <name>
spec:
  containers:
    - name: <name>
      image: myrepo/myimage:latest
      imagePullPolicy: <policy>
      ports:
       - containerPort: 8000
----
+
<1> Label to trigger the webhook.

[NOTE]
====
See the link:https://github.com/openshift/kubernetes-namespace-reservation[kubernetes-namespace-reservation projects]
for an end-to-end example of how to build your own secure and portable webhook admission server
and link:https://github.com/openshift/generic-admission-server[generic-admission-apiserver] for the library.
====

[[architecture-additional-concepts-dynamic-admission-webhooks-examples]]
=== Admission Webhook Example

The following is an example admission webhook that will not allow
link:https://github.com/openshift/kubernetes-namespace-reservation[namespace creation if the namespace is reserved]:

[source,yaml]
----
apiVersion: admissionregistration.k8s.io/v1beta1
  kind: ValidatingWebhookConfiguration
  metadata:
    name: namespacereservations.admission.online.openshift.io
  webhooks:
  - name: namespacereservations.admission.online.openshift.io
    clientConfig:
      service:
        namespace: default
        name: webhooks
       path: /apis/admission.online.openshift.io/v1beta1/namespacereservations
      caBundle: KUBE_CA_HERE
    rules:
    - operations:
      - CREATE
      apiGroups:
      - ""
      apiVersions:
      - "b1"
      resources:
      - namespaces
    failurePolicy: Ignore
----

The following is an example pod that will be
evaluated by the admission webhook named _webhook_:

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  labels:
    role: webhook
  name: webhook
spec:
  containers:
    - name: webhook
      image: myrepo/myimage:latest
      imagePullPolicy: IfNotPresent
      ports:
- containerPort: 8000
----

The following is the front-end service for the webhook:

[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  labels:
    role: webhook
  name: webhook
spec:
  ports:
    - port: 443
      targetPort: 8000
  selector:
role: webhook
----


== Other API objects

.`OAuthClient` Object Definition

[source,yaml]
----
kind: "OAuthClient"
accessTokenMaxAgeSeconds: null <1>
apiVersion: "oauth.openshift.io/v1"
metadata:
  name: "openshift-web-console" <2>
  selflink: "/oapi/v1/oAuthClients/openshift-web-console"
  resourceVersion: "1"
  creationTimestamp: "2015-01-01T01:01:01Z"
respondWithChallenges: false <3>
secret: "45e27750-a8aa-11e4-b2ea-3c970e4b7ffe" <4>
redirectURIs:
  - "https://localhost:8443" <5>

----

<1> The lifetime of access tokens in seconds (see xref:accessTokenMaxAgeSeconds[the description below]).
<2> The `name` is used as the `client_id` parameter in OAuth requests.
<3> When `respondWithChallenges` is set to `true`, unauthenticated requests to
`/oauth/authorize` will result in `WWW-Authenticate` challenges, if supported by
the configured authentication methods.
<4> The value in the `secret` parameter is used as the `client_secret` parameter
in an authorization code flow.
<5> One or more absolute URIs can be placed in the `redirectURIs` section. The
`redirect_uri` parameter sent with authorization requests must be prefixed by
one of the specified `redirectURIs`.

[[accessTokenMaxAgeSeconds]]
The `accessTokenMaxAgeSeconds` value overrides the default `accessTokenMaxAgeSeconds` value in the master configuration file
for individual OAuth clients. Setting this value for a client allows long-lived access tokens for that client
without affecting the lifetime of other clients.

* If `null`, the default value in the master configuration file is used.
* If set to `0`, the token will not expire.
* If set to a value greater than `0`, tokens issued for that client are given the specified expiration time. For example, `accessTokenMaxAgeSeconds: 172800` would cause the token to expire 48 hours after being issued.

.`OAuthClientAuthorization` Object Definition

[source,yaml]
----
kind: "OAuthClientAuthorization"
apiVersion: "oauth.openshift.io/v1"
metadata:
  name: "bob:openshift-web-console"
  resourceVersion: "1"
  creationTimestamp: "2015-01-01T01:01:01-00:00"
clientName: "openshift-web-console"
userName: "bob"
userUID: "9311ac33-0fde-11e5-97a1-3c970e4b7ffe"
scopes: []
----


.`OAuthAuthorizeToken` Object Definition

[source,yaml]
----
kind: "OAuthAuthorizeToken"
apiVersion: "oauth.openshift.io/v1"
metadata:
  name: "MDAwYjM5YjMtMzM1MC00NDY4LTkxODItOTA2OTE2YzE0M2Fj" <1>
  resourceVersion: "1"
  creationTimestamp: "2015-01-01T01:01:01-00:00"
clientName: "openshift-web-console" <2>
expiresIn: 300 <3>
scopes: []
redirectURI: "https://localhost:8443/console/oauth" <4>
userName: "bob" <5>
userUID: "9311ac33-0fde-11e5-97a1-3c970e4b7ffe" <6>

----

<1> `name` represents  the token name, used as an authorization code to exchange
for an OAuthAccessToken.
<2> The `clientName` value is the OAuthClient that requested this token.
<3> The `expiresIn` value is the expiration in seconds from the
creationTimestamp.
<4> The `redirectURI` value is the location where the user was redirected to
during the authorization flow that resulted in this token.
<5> `userName` represents the name of the User this token allows obtaining an
OAuthAccessToken for.
<6> `userUID` represents the UID of the User this token allows obtaining an
OAuthAccessToken for.


.`OAuthAccessToken` Object Definition

[source,yaml]
----
kind: "OAuthAccessToken"
apiVersion: "oauth.openshift.io/v1"
metadata:
  name: "ODliOGE5ZmMtYzczYi00Nzk1LTg4MGEtNzQyZmUxZmUwY2Vh" <1>
  resourceVersion: "1"
  creationTimestamp: "2015-01-01T01:01:02-00:00"
clientName: "openshift-web-console" <2>
expiresIn: 86400 <3>
scopes: []
redirectURI: "https://localhost:8443/console/oauth" <4>
userName: "bob" <5>
userUID: "9311ac33-0fde-11e5-97a1-3c970e4b7ffe" <6>
authorizeToken: "MDAwYjM5YjMtMzM1MC00NDY4LTkxODItOTA2OTE2YzE0M2Fj" <7>

----
<1> `name` is the token name, which is used as a bearer token to authenticate to
the API.
<2> The `clientName` value is the OAuthClient that requested this token.
<3> The `expiresIn` value is the expiration in seconds from the
creationTimestamp.
<4> The `redirectURI` is where the user was redirected to during the
authorization flow that resulted in this token.
<5> `userName` represents the User this token allows authentication as.
<6> `userUID` represents the User this token allows authentication as.
<7> `authorizeToken` is the name of the OAuthAuthorizationToken used to obtain
this token, if any.



.`Identity` Object Definition

[source,yaml]
----
kind: "Identity"
apiVersion: "user.openshift.io/v1"
metadata:
  name: "anypassword:bob" <1>
  uid: "9316ebad-0fde-11e5-97a1-3c970e4b7ffe"
  resourceVersion: "1"
  creationTimestamp: "2015-01-01T01:01:01-00:00"
providerName: "anypassword" <2>
providerUserName: "bob" <3>
user:
  name: "bob" <4>
  uid: "9311ac33-0fde-11e5-97a1-3c970e4b7ffe" <5>
----

<1> The identity name must be in the form providerName:providerUserName.
<2> `providerName` is the name of the identity provider.
<3> `providerUserName` is the name that uniquely represents this identity in the scope of the identity provider.
<4> The `name` in the `user` parameter is the name of the user this identity maps to.
<5> The `uid` represents the UID of the user this identity maps to.


.`User` Object Definition

[source,yaml]
----
kind: "User"
apiVersion: "user.openshift.io/v1"
metadata:
  name: "bob" <1>
  uid: "9311ac33-0fde-11e5-97a1-3c970e4b7ffe"
  resourceVersion: "1"
  creationTimestamp: "2015-01-01T01:01:01-00:00"
identities:
  - "anypassword:bob" <2>
fullName: "Bob User" <3>
----

<1> `name` is the user name used when adding roles to a user.
<2> The values in `identities` are Identity objects that map to this user. May be `null` or empty for users that cannot log in.
<3> The `fullName` value is an optional display name of user.


.`UserIdentityMapping` Object Definition

[source,yaml]
----
kind: "UserIdentityMapping"
apiVersion: "user.openshift.io/v1"
metadata:
  name: "anypassword:bob" <1>
  uid: "9316ebad-0fde-11e5-97a1-3c970e4b7ffe"
  resourceVersion: "1"
identity:
  name: "anypassword:bob"
  uid: "9316ebad-0fde-11e5-97a1-3c970e4b7ffe"
user:
  name: "bob"
  uid: "9311ac33-0fde-11e5-97a1-3c970e4b7ffe"
----

<1> `UserIdentityMapping` name matches the mapped `Identity` name


.`Group` Object Definition

[source,yaml]
----
kind: "Group"
apiVersion: "user.openshift.io/v1"
metadata:
  name: "developers" <1>
  creationTimestamp: "2015-01-01T01:01:01-00:00"
users:
  - "bob" <2>
----

<1> `name` is the group name used when adding roles to a group.
<2> The values in `users` are the names of User objects that are members of this group.



[[persistent-volumes]]
== Persistent volumes

Each PV contains a `spec` and `status`, which is the specification and
status of the volume, for example:

.PV object definition example
[source,yaml]
----
  apiVersion: v1
  kind: PersistentVolume
  metadata:
    name: pv0003
  spec:
    capacity:
      storage: 5Gi
    accessModes:
      - ReadWriteOnce
    persistentVolumeReclaimPolicy: Retain
    nfs:
      path: /tmp
      server: 172.17.0.2

----

[[pv-phase]]
=== Phase

Volumes can be found in one of the following phases:

.Volume phases
[cols="1,2",options="header"]
|===

|Phase |Description

|Available
|A free resource not yet bound to a claim.

|Bound
|The volume is bound to a claim.

|Released
|The claim was deleted, but the resource is not yet reclaimed by the
cluster.

|Failed
|The volume has failed its automatic reclamation.

|===

The CLI shows the name of the PVC bound to the PV.


ifdef::openshift-enterprise,openshift-origin[]
[[pv-mount-options]]
=== Mount options

You can specify mount options while mounting a persistent volume by using the annotation `volume.beta.kubernetes.io/mount-options`.

For example:

.Mount options example
[source, yaml]
----
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv0001
  annotations:
    volume.beta.kubernetes.io/mount-options: rw,nfsvers=4,noexec <1>
spec:
  capacity:
    storage: 1Gi
  accessModes:
  - ReadWriteOnce
  nfs:
    path: /tmp
    server: 172.17.0.2
  persistentVolumeReclaimPolicy: Retain
  claimRef:
    name: claim1
    namespace: default
----
<1> Specified mount options are used while mounting the PV to the disk.

The following persistent volume types support mount options:

- NFS
- GlusterFS
- Ceph RBD
- OpenStack Cinder
- AWS Elastic Block Store (EBS)
- GCE Persistent Disk
- iSCSI
- Azure Disk
- Azure File
- VMWare vSphere

[NOTE]
====
Fibre Channel and HostPath persistent volumes do not support mount options.
====
endif::openshift-enterprise,openshift-origin[]



ifdef::openshift-enterprise,openshift-origin[]
[[block-volume-support]]
== Block volume support
[IMPORTANT]
====
Block volume support is a Technology Preview feature and it is only available for manually provisioned PVs.

ifdef::openshift-enterprise[]
Technology Preview features are not supported with Red Hat production service
level agreements (SLAs), might not be functionally complete, and Red Hat does
not recommend to use them for production. These features provide early access to
upcoming product features, enabling customers to test functionality and provide
feedback during the development process.

For more information about Red Hat Technology Preview features support scope, see
https://access.redhat.com/support/offerings/techpreview/.
endif::[]
====

You can statically provision raw block volumes by including API fields
in your PV and PVC specifications.

To use block volume, you must first enable the `BlockVolume` feature gate. To
enable the feature gates for master(s), add `feature-gates` to
`apiServerArguments` and `controllerArguments`. To enable the feature gates for
node(s), add `feature-gates` to `kubeletArguments`. For example:

----
kubeletArguments:
   feature-gates:
     - BlockVolume=true
----

.PV example
[source, yaml]
----
apiVersion: v1
kind: PersistentVolume
metadata:
  name: block-pv
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  volumeMode: Block <1>
  persistentVolumeReclaimPolicy: Retain
  fc:
    targetWWNs: ["50060e801049cfd1"]
    lun: 0
    readOnly: false
----
<1> `volumeMode` field indicating that this PV is a raw block volume.

.PVC example
[source, yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: block-pvc
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Block <1>
  resources:
    requests:
      storage: 10Gi
----
<1> `volumeMode` field indicating that a raw block persistent volume is requested.

.Pod specification example
[source, yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-block-volume
spec:
  containers:
    - name: fc-container
      image: fedora:26
      command: ["/bin/sh", "-c"]
      args: [ "tail -f /dev/null" ]
      volumeDevices:  <1>
        - name: data
          devicePath: /dev/xvda <2>
  volumes:
    - name: data
      persistentVolumeClaim:
        claimName: block-pvc <3>
----
<1> `volumeDevices` (similar to `volumeMounts`) is used for block devices and can only be used with `PersistentVolumeClaim` sources.
<2> `devicePath` (similar to `mountPath`) represents the path to the physical device.
<3> The volume source must be of type `persistentVolumeClaim` and must match the name of the PVC as expected.


.Accepted values for `VolumeMode`
[cols="1,2",options="header"]
|===

|Value |Default

|Filesystem
|Yes

|Block
|No
|===

.Binding scenarios for block volumes

[cols="1,2,3",options="header"]
|===

|PV VolumeMode |PVC VolumeMode|Binding Result

|Filesystem
|Filesystem
|Bind

|Unspecified
|Unspecified
|Bind

|Filesystem
|Unspecified
|Bind

|Unspecified
|Filesystem
|Bind

|Block
|Block
|Bind

|Unspecified
|Block
|No Bind

|Block
|Unspecified
|No Bind

|Filesystem
|Block
|No Bind

|Block
|Filesystem
|No Bind
|===

[IMPORTANT]
====
Unspecified values result in the default value of *Filesystem*.
====
endif::openshift-enterprise,openshift-origin[]


.PVC object definition example
[source,yaml]
----
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: myclaim
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 8Gi
  storageClassName: gold

----


.Mount volume to the host and into the pod example
[source,yaml]
----
kind: Pod
apiVersion: v1
metadata:
  name: mypod
spec:
  containers:
    - name: myfrontend
      image: dockerfile/nginx
      volumeMounts:
      - mountPath: "/var/www/html"
        name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: myclaim
----

== Image streams

