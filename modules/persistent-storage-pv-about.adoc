// Module included in the following assemblies:
//
// * architecture/storage.adoc

[id='persistent-storage-pv-about-{context}']
= About PersistentVolumes

Each PersistentVolumes (PV) contains a `spec` and `status`, which is the
specification and status of the volume.

Generally, a PV has a specific storage capacity. This is set by using the PV's
`capacity` attribute.

Currently, storage capacity is the only resource that can be set or requested.
Future attributes may include IOPS, throughput, and so on.

A `PersistentVolume` can be mounted on a host in any way supported by the
resource provider. Providers will have different capabilities and each PV's
access modes are set to the specific modes supported by that particular volume.
For example, NFS can support multiple read/write clients, but a specific NFS PV
might be exported on the server as read-only. Each PV gets its own set of access
modes describing that specific PV's capabilities.

Claims are matched to volumes with similar access modes. The only two matching
criteria are access modes and size. A claim's access modes represent a request.
Therefore, you might be granted more, but never less. For example, if a claim
requests RWO, but the only volume available is an NFS PV (RWO+ROX+RWX), the
claim would then match NFS because it supports RWO.

Direct matches are always attempted first. The volume's modes must match or
contain more modes than you requested. The size must be greater than or equal to
what is expected. If two types of volumes (NFS and iSCSI, for example) have
the same set of access modes, either of them can match a claim with those
modes. There is no ordering between types of volumes and no way to choose one
type over another.

All volumes with the same modes are grouped, and then sorted by size (smallest to
largest). The binder gets the group with matching modes and iterates over each
(in size order) until one size matches.

.PV access modes
[cols="1,1,3",options="header"]
|===
|Access Mode |CLI abbreviation |Description
|ReadWriteOnce
|`RWO`
|The volume can be mounted as read-write by a single node.
|ReadOnlyMany
|`ROX`
|The volume can be mounted read-only by many nodes.
|ReadWriteMany
|`RWX`
|The volume can be mounted as read-write by many nodes.
|===

[IMPORTANT]
====
A volume's `AccessModes` are descriptors of the volume's capabilities. They
are not enforced constraints. The storage provider is responsible for runtime
errors resulting from invalid use of the resource.

For example, Ceph offers *ReadWriteOnce* access mode. You must
mark the claims as `read-only` if you want to use the volume's
ROX capability. Errors in the provider show up at runtime as mount errors.
ifdef::openshift-enterprise,openshift-origin[]

iSCSI and Fibre Channel volumes do not currently have any fencing mechanisms. You must
ensure the volumes are only used by one node at a time. In certain situations,
such as draining a node, the volumes can be used simultaneously by two nodes.
Before draining the node, first ensure the pods that use these volumes are
deleted.
endif::openshift-enterprise,openshift-origin[]
====

.Supported access modes for PVs
[cols=",^v,^v,^v", width="100%",options="header"]
|===
|Volume Plug-in  |ReadWriteOnce  |ReadOnlyMany  |ReadWriteMany
|AWS EBS  | ✅ | - |  -
|Azure File | ✅ | ✅ | ✅
|Azure Disk | ✅ | - | -
|Ceph RBD  | ✅ | ✅ |  -
|Fibre Channel  | ✅ | ✅ |  -
|GCE Persistent Disk  | ✅ | - |  -
|GlusterFS  | ✅ | ✅ | ✅
|HostPath  | ✅ | - |  -
|iSCSI  | ✅ | ✅ |  -
|NFS  | ✅ | ✅ | ✅
|Openstack Cinder  | ✅ | - |  -
|VMWare vSphere | ✅ | - |  -
|Local | ✅ | - |  -
|===

[NOTE]
====
Use a recreate deployment strategy for pods that rely on AWS EBS, GCE
Persistent Disks, or Openstack Cinder PVs.
====

ifdef::openshift-dedicated,openshift-online[]
[id='persistent-storage-pv-restrictions-{context}']
== Restrictions

The following restrictions apply when using persistent volumes with {product-title}:
endif::[]

ifdef::openshift-dedicated[]
[IMPORTANT]
====
 * PVs are provisioned with either EBS volumes (AWS) or GCP storage (GCP), depending on where the cluster is provisioned.
 * Only RWO access mode is applicable, as EBS volumes and GCE Persistent Disks cannot be mounted to multiple nodes.
 * *emptyDir* has the same lifecycle as the pod:
   ** *emptyDir* volumes survive container crashes/restarts.
   ** *emptyDir* volumes are deleted when the pod is deleted.
====
endif::[]

ifdef::openshift-online[]
[IMPORTANT]
====
 * PVs are provisioned with EBS volumes (AWS).
 * Only RWO access mode is applicable, as EBS volumes and GCE Persistent Disks cannot be mounted to multiple nodes.
 * Docker volumes are disabled.
   ** VOLUME directive without a mapped external volume fails to be instantiated.
 * *emptyDir* is restricted to 512 Mi per project (group) per node.
   ** A single pod for a project on a particular node can use up to 512 Mi of *emptyDir* storage.
   ** Multiple pods for a project on a particular node share the 512 Mi of *emptyDir* storage.
 *  *emptyDir* has the same lifecycle as the pod:
   ** *emptyDir* volumes survive container crashes/restarts.
   ** *emptyDir* volumes are deleted when the pod is deleted.
====
endif::[]

[id='persistent-storage-pv-reclaim-policy-{context}']
== Reclaim policy

The following table lists current reclaim policies:

.Current reclaim policies
[cols="1,2",options="header"]
|===

|Reclaim policy |Description

|Retain
|Manual reclamation

|===

[WARNING]
====
If you do not want to retain all pods, use dynamic provisioning.
====